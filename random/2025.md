### silent failures 

It's important to acknowledge *why* things go wrong, instead of acknowledging a failure and just move on with life. People don't always do that, but it's actually important (especially when working in a group, especially when working in a group of *opinionated people*). Being aware of why things go wrong helps us understand how to avoid similar failures. 
It seems like people don't always *actually* want what they claim they want: it's important to reflect on *actions* besides words. 
If you say you want something, but don't get it when presented with an opportunity, it might be the case to reflect on *why* you want it and what you *expect* from it. 
We all see benefits in the things we want, it's important to reflect on what these are and what can satisfy them.

The best way to do something is to actually start doing it.

### acl2

ACL2 is a very nice proof assistant, that contains a bunch of libraries (`books`), each of which contains theorems useful for formal verification. 
In particular, they have a tool to verify SystemVerilog designs by extracting a FSM from the design (the FSM format is called `svex`) such that it can be fed to a SAT solver (Hunt 2006). 

ACL2 works very well with emacs (which is initially a pain in the neck, but is actually worth it!)

The TLDR to use ACL2 for hardware verification is quite straightforward: there are no variables, we call one by one the functions we want to apply and store them in *constants*, which 
we can then use as arguments for other functions. I like to think about it incrementally: once you call a function, its result will continue existing in your scope and is immutable.

A short guide to extract a FSM from a SystemVerilog design: 

1. load the books containing the functions we'll need:

    (include-book "centaur/sv/top" :dir :system)
    (include-book "centaur/vl/loader/top" :dir :system)
    (include-book "centaur/vl/top-doc" :dir :system)
    (include-book "quicklisp/top" :dir :system)
    (include-book "oslib/top" :dir :system)

2. load the SystemVerilog file in *vl-design*

    (defconsts
    (*vl-design* state)
    (b*
        (((mv loadresult state)
        (vl::vl-load (vl::make-vl-loadconfig :start-files (list "../simple-design.sv")))))
        (mv (vl::vl-loadresult->design loadresult)
            state)))
        
3. check that no parsing errors occurred

    (cw-unformatted (vl::vl-reportcard-to-string
                     (vl::vl-design-reportcard *vl-design*)))

4. convert the loaded module into a FSM

    (defconsts
        (*osvex-design* *simplified-good*
                            *simplified-bad*)
        (b* (((mv errmsg svex-design good bad)
            (vl::vl-design->sv-design "aes_ctr_fsm" *vl-design*
                                        (vl::make-vl-simpconfig))))
        (and errmsg (raise "~@0~%" errmsg))
        (mv svex-design good bad)))

5. check the output of the conversion 

    (cw-unformatted
    (vl::vl-pps-modulelist (vl::vl-design->mods *simplified-bad*)))

    (cw-unformatted
    (vl::vl-pps-modulelist (vl::vl-design->mods *simplified-good*)))

6. print the whole svex design 

    (without-evisc *svex-design*)

7. create channel for output redirection 

    (mv-let (chan state)
            (open-output-channel "simple-design.out" :character state)
            (set-proofs-co chan state))

8. redirect stdout to that channel

    (set-standard-co (proofs-co state) state)

9. compile design to fsm

    (sv::svex-design-compile *svex-design*)

-> use (set-iprint t) to enable debug mode!
Emacs has a lot of commands, knowing them is quite useful.

### `omega` tactic in lean4

`omega` is a very cool and powerful tactic in the Lean framework, made to solve linear optimization problems. 
Consider the following clauses: 
EX
Clearly, there is no x that can satisfy both these parameters: `omega` will tell us that it could find no x satisfying both.
Another example: 
EX
There exists (at least... depending on our domain of interest :p) *at least* one x satisfying both constraints. 
We say that the problem is *satisfiable* and sleep tight at night knowing that this problem exists. 

Now if we consider non-linear functions, `omega` won't work as expected, e.g. : 
EX
What `omega` does in this case is to assign a dummy name to x ^ 2, considering it another variable of the problem: 
EX
Knowing about these assignments is very important to reason about more complex functions, whose constraints are known, e.g.:
EX
In this case `omega` won't be able to solve the problem, since it does not know power and does not know that 2 ^ w is strictly positive. 
However, if we add: 
EX
since the variable assigned to 2 ^ w is anyways constrained enough.

### intro to cat theory 

model something 
    . sets, functions between sets
    . vec spaces, maps between vec spaces (ps. what is a vector space?)
    . types, function between types (e.g. in Lean)
        e.g. A \oplus B ~ inductive Sum A B 
                            | inj_l : A => Sum A B
                            | inj_r : B => Sum A B 
    . groups, group homorphisms 
    . dialects in MLIR

category := 
        . collection of objects O (O : Set, e.g. O is everything that lives in Type)
        . collection of morphisms \forall o1, o2 \in O : Hom(o1, o2) (or: Hom: O x O --> Set)

    homomorphism := map between two algebraic structures of the same type that preserves the operations of the structures

    we define: 
        . composition operation "o" : \forall A, B, C \in O : Hom (B, C) x Hom (A, B) --> Hom (A, C) (recall: h (*) = g o f (*) = g(f(*)))
        . identity : \forall A \in O : \exists IdA \in Hom (A, A) : \forall (g: B --> A) IdA o g = g
                                                                    \forall (h: A --> B) h o IdA = h
        . associativity : \forall (f g h : ) 
                    (f o g) o h : f (g o h)

    examples: 
        . O = {X, Y, Z, ...} collection of sets
            given X, Y \in O : Hom (X, Y) = {f : X --> Y}
            o : function composition 
            \forall A \in O : IdA = [x \in A |-> x ]
            product for this collection of objects can be proven to be equivalent to the cartesian product 
        . O = {X, Y, Z, ...} collection of natural numbers 
            Hom (X, Y) = { "set containing a single empty set {*} | x \le y, \empty otherwise} where {*} is singleton 
            Hom (Y, Z) o Hom (X, Y) : Hom (X, Z) "Y \le Z o X \le Y = X \le Z"
        . O = {A, B, C, ...} collection of natural numbers 
            Hom (A, B) = "matrices of dimension A x B"
            Hom (B, C) o Hom (A, B) : matrix multiplication 
            IdA = identity matrix A x A (Hom (A, A))
        . O = {P, Q, R, ...} collection of propositions about natural numbers 
            Hom (P, Q) = set of all proofs that P => Q (note that false implies everything)
            Hom (Q, R) o Hom (P, Q) : P => Q => R (chain composition)
            in this case one product can be conjunction (by the def of product)
        
    we define product operation "x": \forall C s.t. \sA : C --> A, \sB : C --> B \exists! \t : C --> A x B (note that the product is unique *modulo* isomorphisms.)

                    ------- C --------
                    |       :\t      |              note that \t is unique!
                    |       :        |              if this happens we say "the diagram commutes"
                    |       v        |              eg1: \sA := \pA o \t 
                \sA |     A x B      | \sB               \sB := \pB o \t
                    | \pA |   | \pB  |              eg2: 
                    |     |   |      |
                    |     v   v      |
                    ----> A   B <-----

    in the natural number with \le  example: 

                    n       m       . k \le n
                    |\     /|       . k \le m 
                    | \   / |       . \forall l, l \le n \land l \le m => l \le k  
                    |  \ /  |
                    |   k   |
                    |   ^   |
                    |   :   |
                    --- l ---

    eg1:            4       9       . 3 \le 4
                    |\     /|       . 3 \le 9 
                    | \   / |       . \forall l, l \le 4 \land l \le 9 => l \le 3 : this is not correct, since e.g. l = 4 does not satisfy 4 \le 3
                    |  \ /  |
                    |   3   |
                    |   ^   |
                    |   :   |
                    --- 2 ---

    eg2:            4       9       . 4 \le 4
                    |\     /|       . 4 \le 9 
                    | \   / |       . \forall l, l \le 4 \land l \le 9 => l \le 3 : this is correct, since *any number* that is \le 4 (0, 1, 2, 3)
                    |  \ /  |         and \le 9 (0, 1, 2, 3, 4, 5, 6, 7, 8) is also \le 4 
                    |   4   |
                    |   ^   |
                    |   :   |
                    --- 2 ---

    eg3:            P       Q       . (P & Q) => P
                    |\     /|       . (P & Q) => Q
                    | \   / |       . \forall R, (R => P \land R => Q) => (R => P & Q) : this is correct, since *any prop* that implies P and Q
                    |  \ /  |         also implies P & Q
                    | P & Q |
                    |   ^   |
                    |   :   |
                    --- R ---
    
    eg4: on vector spaces?

    "everything is just a morphism"

    "a product is the limit of some shape and some common shapes are given names" : meaning that asking the limits of a shape unravels to the definition of a product 

    not every category has products! if something satisfying those properties exists we call it a property, but that does not always happen. 

    eg4: O = "category of sets of card \le 2" -> does not have all products 
        consider A = {\empty, {\empty}} and B = {\empty, {\empty}}
                we know that the product should have four elements! but the category O only contains elements with size \le 2, so this such product does not exist 


vector space := a set whose elements, often called vectors, can be added together and multiplied ("scaled") by numbers called scalars. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. Real vector spaces and complex vector spaces are kinds of vector spaces based on different kinds of scalars: real numbers and complex numbers. Scalars can also be, more generally, elements of any field. 

colimit := like product but arrows go in the opposite direction: 

                    ------> D <-------
                    |       ^\t      |              
                    |       :        |              
                    |       :        |              
                \sA |       Q        | \sB          
                    | \pA ^   ^ \pB  |              
                    |     |   |      |
                    ----- A   B ------
    
            for sets this corresponds to disjoint union ("smallest thing")

    an example I don't understand:

                 f
              A --> B 
            g |     :
              v     v
              C .. > B |_| C / f ~ g ("B disjoint union C quotiented by ~")

    --> the product is unique modulo isomorphisms


    another example I don't understand: 


             C ---- D 
            A        E              ===>                E,A         or          A,C - D,E
            |       /                                 /  |                       |  /
            |      /                               F,D - B,C                     B,F
            |     /
            B    F

    and defining different colimits gives you different ways to glue up these things and define different shapes (e.g. torus)
    --> you need category theory to discuss the glueing

### my research 

Formal verification is well known to be an intense task in the hardware design pipeline, taking up to 50% of the total design time (cite here). 
In particular, the verification and testing of designs is a task typically taking place at the end of the compilation, with compiled designs consisting of 
very big, detailed and low-level representations. 
The size of these low-level representations make it hard to properly exploit formal methods for these tasks, as they tend to not scale well 
with the size and complexity of modern hardware designs. 
As hardware compilers shift towards the use of multiple IRs and lowering to compile a design (e.g. CIRCT), compilers generate
various representations after different lowering lowerings, representations which are immediately discarded. 
We propose using these pieces of information to improve the scalability and usability of formal verification tooling for modern hardware design. 

In particular, to make formal methods fully accessible from and embedded with the hardware design process, I will work on two complementary approaches.
*distributing the verification efforts along the compilation pipeline*w
We propose building a mapping between the semantics of the IRs 
modern compilers rely on and formal verification tooling, e.g. by representing an IR operation as an operation on Bitvectors and transforming
the problem of verifying a property at a certain IR into a bitvector problem. 
At this stage, it is very important to make sure that the semantics of the IRs is correctly encoded as something the formal verification tool can 
easily digest, to make the verification effactive. In fact, if this mapping is naive, the solver might not be able to be efficient, as 
our previous work on the verification of FSMs suggested. 
*improving the efficiency and effectiveness of formal verification tooling*
Given the complexity of modern hardware designs and compilation toolchains, it is important to make sure that the solvers are effective and support numerous 
data structures and decision procedures, to be capable of effectively reason about IRs representing different abstractions.

Overall, by tackling these two aspects, we will be able to (1) verify the correctness of the compiler, by reasoning about the semantics of different IRs, 
(2) distribute the verification overhead along the entire compilation toolchain, making sure that properties are checked as the design is compiled, 
exploiting the data the compiler would produce anyways, (3) decrease the overhead of bottom-line verification tooling, as the compiled design is partially 
verified already.

###  lean-MLIR framework 

### bool-prop coercion in lean 

### knaster-tarski theorem

### lambda calculus